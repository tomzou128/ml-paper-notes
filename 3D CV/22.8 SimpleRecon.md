Neural Recon



## 代码实现

### 环境配置

```
cuda=11.3
pytorch=1.12.1
```





准备 GT





Fusion

使用 scannet v2 时会用





读取数据

**创建数据集**

SimpleRecon 中所有的数据集类继承自 `GenericMVSDataset` 并由它进行统一创建，有以下参数：

```py
dataset_path,						# 数据集路径，'F:\\D\\ScanNetv1'
split,								# 数据模式，'train', 'val' or 'test'
mv_tuple_file_suffix,				# pair 文件的后缀，与数据模式拼接成为 pair 文件全名
tuple_info_file_location=None,		# pair 文件的文件夹路径，拼接用，'data_splits/ScanNetv1/'
limit_to_scan_id=None,				# 一个场景名称，如有则数据集对象只包含该场景的样本
num_images_in_tuple=None,			# pair 的数量，默认为8，1个ref，7个src
image_height=384,					# 传入模型的图片尺寸，自动缩放
image_width=512,
include_high_res_color=False,		# 是否保留高尺寸图片，色彩融合或深度可视化时为 True
high_res_image_width=640,			# 保留高尺寸图后的尺寸，通常比传入模型的图片尺寸大
high_res_image_height=480,
image_depth_ratio=2,				# 目标深度图的尺寸和索引0的内参矩阵对应的尺寸是 image/ratio，默认为图片是深度图的2倍
include_full_res_depth=False,		# 是否保留原尺寸深度图，没有尺寸限制
include_full_depth_K=False,			# 是否保留原尺寸深度图的内参矩阵，键 'high_res_color_b3hw'
color_transform=transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),
shuffle_tuple=False,				# 是否随机src样本的顺序，仅用于验证根据pose排src样本的有效性
pass_frame_id=False,				# 是否保留 frame_id
skip_frames=None,					# 略过的图片
skip_to_frame=None,					# 跳到图片
verbose_init=True,					# 创建时是否打印信息
native_depth_width=640,				# 没有在 ScanNet 中使用
native_depth_height=480,			# 没有在 ScanNet 中使用
image_resampling_mode=pil.BILINEAR
```

创建时读取 pair 文件，然后筛选出本场景的 pair。

```py
tuple_information_filepath = os.path.join(tuple_info_file_location, 
                                            f"{split}{mv_tuple_file_suffix}")
self.frame_tuples = readlines(tuple_information_filepath)

# Filter the pairs of this scene, because Dataset object is for one scene
if limit_to_scan_id is not None:
    self.frame_tuples = [frame_tuple for frame_tuple in 
            self.frame_tuples if limit_to_scan_id == 
                frame_tuple.split(" ")[0]]
```

然后计算深度图的尺寸，用在读取深度图和内参矩阵。

```py
# size up depth using ratio of RGB to depth
# 384/2 = 192, 512/2 = 256
self.depth_height = self.image_height // image_depth_ratio
self.depth_width = self.image_width // image_depth_ratio
```



**读取样本**

`cur_data`：ref 图片

```py
'world_T_cam_b44': 	原本的 pose 矩阵
'cam_T_world_b44': 	pose 矩阵的逆
'image_b3hw':		图片，进行标准化，训练时加数据增强，尺寸是声明的，如 384*512
'K_full_depth_b44': 可选，原尺寸深度图的内参矩阵，还会计算内参矩阵的逆，就不在此写出
'K_s0_b44':		最大尺寸的深度图的内参矩阵，尺寸是图片尺寸的1/2，深度图尺寸的1/1
'K_s1_b44':		内参矩阵，尺寸是图片尺寸的1/4，深度图尺寸的1/2
'K_s2_b44':		内参矩阵，尺寸是图片尺寸的1/8，深度图尺寸的1/4
'K_s3_b44':		内参矩阵，尺寸是图片尺寸的1/16，深度图尺寸的1/8
'K_s3_b44':		内参矩阵，尺寸是图片尺寸的1/32，深度图尺寸的1/16
'depth_b1hw':	深度图，尺寸是 image_width/ratio，如 192*256
'mask_b1hw':	浮点数类型的 mask，遮盖超出深度范围的像素，用 np.nan 遮盖
'mask_b_b1hw':	布朗类型的 mask，遮盖超出深度方位的像素
'high_res_color_b3hw':	可选，高尺寸的图片，进行标准化
'full_res_depth_b1hw':	可选，原尺寸深度图
'full_res_mask_b1hw':	可选，浮点数类型的 mask，遮盖超出深度范围的像素，用 nan 遮盖
'full_res_mask_b_b1hw':	可选，布朗类型的 mask，遮盖超出深度方位的像素
'frame_id_string':		可选，图片的名称，不含场景名
```

`src_data`：7个 src 图片，每张图的内容与上面一样，但顺序根据它们和 ref 图片的 pose 距离排序，越近的越考前。



### 创建模型



### 前向过程

