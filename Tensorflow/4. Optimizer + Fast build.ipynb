{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "\n",
    "1. 各种动态梯度的介绍和代码。\n",
    "2. 使用Keras快速搭建神经网络。\n",
    "3. 使用Keras和自定义类搭建神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common structure\n",
    "已有参数的梯度：grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设定：\n",
    "############################\n",
    "\n",
    "############################\n",
    "for epoch in range(epoch):  # 数据集级别的循环，每个epoch循环一次数据集\n",
    "    for step, (x_train, y_train) in enumerate(train_db):  # batch级别的循环 ，每个step循环一个batch\n",
    "        with tf.GradientTape() as tape:  # with结构记录梯度信息\n",
    "            y = tf.matmul(x_train, w1) + b1  # 神经网络乘加运算\n",
    "            y = tf.nn.softmax(y)  # 使输出y符合概率分布（此操作后与独热码同量级，可相减求loss）\n",
    "            y_ = tf.one_hot(y_train, depth=3)  # 将标签值转换为独热码格式，方便计算loss和accuracy\n",
    "            loss = tf.reduce_mean(tf.square(y_ - y))  # 采用均方误差损失函数mse = mean(sum(y-out)^2)\n",
    "            loss_all += loss.numpy()  # 将每个step计算出的loss累加，为后续求loss平均值提供数据，这样计算的loss更准确\n",
    "        # 计算loss对各个参数的梯度\n",
    "        grads = tape.gradient(loss, [w1, b1])\n",
    "\n",
    "        # 实现梯度更新：\n",
    "        ############################\n",
    "\n",
    "        ############################\n",
    "\n",
    "\n",
    "    # 每个epoch，打印loss信息\n",
    "    print(\"Epoch {}, loss: {}\".format(epoch, loss_all / 4))\n",
    "    train_loss_results.append(loss_all / 4)  # 将4个step的loss求平均记录在此变量中\n",
    "    loss_all = 0  # loss_all归零，为记录下一个epoch的loss做准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度更新\n",
    "w1.assign_sub(lr * grads[0])  # 参数w1自更新\n",
    "b1.assign_sub(lr * grads[1])  # 参数b自更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDM\n",
    "在SGD基础上加入动量/历史影响。貌似有多种实现方法？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 额外参数设定\n",
    "m_w, m_b = 0, 0\n",
    "beta = 0.9\n",
    "\n",
    "# 梯度更新\n",
    "m_w = beta * m_w + (1 - beta) * grads[0]\n",
    "m_b = beta * m_b + (1 - beta) * grads[1]\n",
    "w1.assign_sub(lr * m_w)\n",
    "b1.assign_sub(lr * m_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adagrad\n",
    "加入二次导数，在鞍点也可以继续移动。当前梯度过大时加以限制，过小时保持梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 额外参数设定\n",
    "v_w, v_b = 0, 0\n",
    "\n",
    "# 梯度更新\n",
    "v_w += tf.square(grads[0])\n",
    "v_b += tf.square(grads[1])\n",
    "w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))\n",
    "b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rmsprop\n",
    "在Adagrad基础上加入二次导数的动量/历史影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 额外参数设定\n",
    "v_w, v_b = 0, 0\n",
    "beta = 0.9\n",
    "\n",
    "# 梯度更新\n",
    "v_w = beta * v_w + (1 - beta) * tf.square(grads[0])\n",
    "v_b = beta * v_b + (1 - beta) * tf.square(grads[1])\n",
    "w1.assign_sub(lr * grads[0] / tf.sqrt(v_w))\n",
    "b1.assign_sub(lr * grads[1] / tf.sqrt(v_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "结合SGDM和Rmsprop，有一次导数和二次导数，有动量，并有超参数辅助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 额外参数设定\n",
    "m_w, m_b = 0, 0\n",
    "v_w, v_b = 0, 0\n",
    "beta1, beta2 = 0.9, 0.999\n",
    "delta_w, delta_b = 0, 0\n",
    "global_step = 0\n",
    "\n",
    "# 梯度更新\n",
    "global_step += 1\n",
    "\n",
    "m_w = beta1 * m_w + (1 - beta1) * grads[0]\n",
    "m_b = beta1 * m_b + (1 - beta1) * grads[1]\n",
    "v_w = beta2 * v_w + (1 - beta2) * tf.square(grads[0])\n",
    "v_b = beta2 * v_b + (1 - beta2) * tf.square(grads[1])\n",
    "\n",
    "m_w_correction = m_w / (1 - tf.pow(beta1, int(global_step)))\n",
    "m_b_correction = m_b / (1 - tf.pow(beta1, int(global_step)))\n",
    "v_w_correction = v_w / (1 - tf.pow(beta2, int(global_step)))\n",
    "v_b_correction = v_b / (1 - tf.pow(beta2, int(global_step)))\n",
    "\n",
    "w1.assign_sub(lr * m_w_correction / tf.sqrt(v_w_correction))\n",
    "b1.assign_sub(lr * m_b_correction / tf.sqrt(v_b_correction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use keras to build neural network\n",
    "6 steps:\n",
    "1. Import libraries <br>\n",
    "2. Build training set and testing set <br>\n",
    "3. model = tf.keras.models.Sequential <br>\n",
    "4. model.compile <br>\n",
    "5. model.fit <br>\n",
    "6. model.summary <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct neural network structure\n",
    "```model = tf.keras.models.Sequential([<network structure>])```\n",
    "\n",
    "* 拉直层： <br>\n",
    "```tf.keras.layers.Flatten()```\n",
    "\n",
    "\n",
    "* 全连接层： <br>\n",
    "```tf.keras.layers.Dense(神经元个数, activation = \"激活函数\", kernal_regularizer = 正则化方法)```  <br>\n",
    "activation（字符串给出）可选: relu、softmax、sigmoid 、tanh <br>\n",
    "kernel_regularizer可选: ```tf.keras.regularizers.l1()```、```tf.keras.regularizers.l2()``` <br>\n",
    "\n",
    "\n",
    "* 卷积层： <br>\n",
    "```tf.keras.layers.Conv2D(filters = 卷积核个数, kernel_size = 卷积核尺寸, strides = 卷积步长， padding = \" valid\" or \"same\")```\n",
    "\n",
    "\n",
    "* LSTM层： <br>\n",
    "```tf.keras.layers.LSTM()```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select optimizer and loss function\n",
    "```model.compile(optimizer = 优化器, loss = 损失函数, metrics = [“准确率”] )```\n",
    "\n",
    "**Optimizer可选(使用函数可以调参数):** <br>\n",
    "* 'sgd' or ```tf.keras.optimizers.SGD (lr=学习率,momentum=动量参数)``` <br>\n",
    "* 'adagrad' or ```tf.keras.optimizers.Adagrad (lr=学习率)``` <br>\n",
    "* 'adadelta' or ```tf.keras.optimizers.Adadelta (lr=学习率)``` <br>\n",
    "* 'adam' or ```tf.keras.optimizers.Adam (lr=学习率, beta_1=0.9, beta_2=0.999)``` <br>\n",
    "\n",
    "\n",
    "**loss可选:** <br>\n",
    "* Mean square error <br>\n",
    "```tf.keras.losses.MeanSquaredError()```\n",
    "  \n",
    "  \n",
    "* Binary cross entropy <br>\n",
    "Use this cross-entropy loss when there are only two label classes (assumed to be 0 and 1) <br>\n",
    "```tf.keras.losses.BinaryCrossentropy(from_logits = )```\n",
    "  \n",
    "  \n",
    "* Categorical cross entroty <br>\n",
    "Use this cross-entropy loss when the labels were provided in a one-hot representation. <br>\n",
    "```tf.keras.losses.CategoricalCrossentropy(from_logits = )```\n",
    "  \n",
    "  \n",
    "* Sparse Categorical cross entropy <br>\n",
    "Use this cross-entropy loss when the labels were provided in integer representation. <br>\n",
    "```tf.keras.losses.SparseCategoricalCrossentropy(from_logits = )```\n",
    "\n",
    "  \n",
    "```from_logits = True```: The model's output didn't go through sigmoid or softmax <br>\n",
    "```from_logits = False```: The model's ouput went through sigmoid or softmax, now in probability distribution format. <br>\n",
    "\n",
    "\n",
    "**Metrics可选:** <br>\n",
    "* 'accuracy' ：y_和y都是数值，如y_=[1] y=[1] <br>\n",
    "* 'categorical_accuracy' ：y_和y都是独热码(概率分布)，如y_=[0,1,0] y=[0.256,0.695,0.048] <br>\n",
    "* 'sparse_categorical_accuracy' ：y_是数值，y是独热码(概率分布),如y_=[1] y=[0.256,0.695,0.048] <br>\n",
    "\n",
    "Metrics 选择 'accuracy' 即可，TF会自动转化成 'BinaryAccarcy', 'CategoricalAccuracy' 或 'SparseCategoricalAccuracy' 其中一种。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start fitting\n",
    "```\n",
    "model.fit (训练集的输入特征, 训练集的标签, batch_size= , epochs= ,\n",
    "validation_data = (测试集的输入特征，测试集的标签),\n",
    "validation_split = 从训练集划分多少比例给测试集, \n",
    "validation_freq = 多少次epoch测试一次)\n",
    "```\n",
    "```validation_data``` 和 ```validation_split``` 二选一。\n",
    "\n",
    "所有详见：https://keras.io/zh/models/sequential/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction\n",
    "* ```model()```<br>\n",
    "Computation is done in batches (4D). Return in ```tf.tensor```.\n",
    "\n",
    "\n",
    "* ```model.predict()```<br>\n",
    "```\n",
    "predict(x, batch_size=None, verbose=0, steps=None, callbacks=None, max_queue_size=10,workers=1, use_multiprocessing=False)\n",
    "```\n",
    "Computation is done in batches (4D). Return in ```numpy array```. This method is designed for performance in large scale inputs. For small amount of inputs that fit in one batch, directly using __call__ is recommended for faster execution, e.g., ```model(x)```, or ```model(x, training=False)```\n",
    "\n",
    "\n",
    "* ```model.predict()```<br>\n",
    "Returns predictions for a single batch of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast build example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'sparse_categorical_accuracy', 'val_loss', 'val_sparse_categorical_accuracy'])\n",
      "loss:  0.3332972486813863\n",
      "sparse_categorical_accuracy:  0.96666664\n",
      "val_loss:  0.40018776059150696\n",
      "val_sparse_categorical_accuracy:  1.0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              multiple                  15        \n",
      "=================================================================\n",
      "Total params: 15\n",
      "Trainable params: 15\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "x_train = datasets.load_iris().data\n",
    "y_train = datasets.load_iris().target\n",
    "\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(x_train)\n",
    "np.random.seed(116)\n",
    "np.random.shuffle(y_train)\n",
    "tf.random.set_seed(116)\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(3, activation = 'softmax', kernel_regularizer = tf.keras.regularizers.l2())\n",
    "])\n",
    "\n",
    "model.compile(optimizer = tf.keras.optimizers.SGD(lr=0.1),\n",
    "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics = ['sparse_categorical_accuracy'])\n",
    "\n",
    "# By default, fit() will print out the progress of every iteration, set verbose = 0 can disable print out.\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20, verbose = 0)\n",
    "# 进行了500次迭代，所以有500次数据，取最后一次显示。\n",
    "print(history.history.keys())\n",
    "print('loss: ', history.history['loss'][-1])\n",
    "print('sparse_categorical_accuracy: ', history.history['sparse_categorical_accuracy'][-1])\n",
    "print('val_loss: ', history.history['val_loss'][-1])\n",
    "print('val_sparse_categorical_accuracy: ', history.history['val_sparse_categorical_accuracy'][-1])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graph (Complete procedure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-63f58b2c91fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sparse_categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_sparse_categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print(history.history.keys())\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast build + customized class example\n",
    "### Class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisModel(Model):\n",
    "    def __init__(self):\n",
    "        super(IrisModel, self).__init__()\n",
    "        # Define all the modules used\n",
    "        self.d1 = Dense(3, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.l2())\n",
    "    \n",
    "    def call(self, x):\n",
    "        # Define the forward propagation structure\n",
    "        y = self.d1(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue\n",
    "model = IrisModel()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=0.1),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "h = model.fit(x_train, y_train, batch_size=32, epochs=500, validation_split=0.2, validation_freq=20, , verbose = 0)\n",
    "print('loss: ', h.history['loss'][-1])\n",
    "print('sparse_categorical_accuracy: ', h.history['sparse_categorical_accuracy'][-1])\n",
    "print('val_loss: ', h.history['val_loss'][-1])\n",
    "print('val_sparse_categorical_accuracy: ', h.history['val_sparse_categorical_accuracy'][-1])\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}